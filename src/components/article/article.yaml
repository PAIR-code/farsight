intro:
  - >
    Prompt-based interfaces (e.g., <a
    href='https://aistudio.google.com/app/prompts/new_chat' target='_blank'
    rel='noreferrer'>Google AI Studio</a>, <a
    href='https://platform.openai.com/playground' target='_blank'
    rel='noreferrer'>OpenAI Playground</a>, and <a
    href='https://poloclub.github.io/wordflow/' target='_blank'
    rel='noreferrer'>Wordflow</a>)  for Large Language Models (LLMs) have made
    prototyping and building AI-powered applications easier than ever before.
    However, identifying potential harms that may arise from AI applications
    remains a challenge, particularly during prompt-based prototyping. To
    address this challenge, we designed and developed Farsight, a novel
    interactive <em>in situ</em> tool that helps people <strong>identify potential harms</strong>
    from the AI applications they are prototyping.

  - >
    Farsight is designed and developed for both AI prototypers (i.e., prompt
    creators) and developers of web-based prompting interfaces. First, AI
    prototypers can use Farsight in various ways: through this demo website, a
    <a href='https://github.com/PAIR-code/farsight' target='_blank'
    rel='noreferrer'>Chrome Extension</a> enabling it on Google AI Studio, or
    using a <a href='https://pypi.org/project/farsight/' target='_blank'
    rel='noreferrer'>Python package</a> to enable it on Jupyter Notebook.
    Developers of prompting tools can easily integrate our open-source and
    reusable components into their AI prototyping tools.

usageIntro:
  - >
    Farsight includes a collection of <strong><em>in situ</em> widgets</strong>
    that can be integrated into diverse prompting workflows, such as web-based
    prompting GUIs and computational notebooks. To ensure users remain focused
    on their primary task or of writing prompting, Farsight uses progressive
    disclosure to gradually reveal these widgets with increasing complexity and
    engagement: (1) Alert Symbol → (2) Awareness Sidebar → (3) Harm
    Envisioner.

usageAlert:
  - >
    The Alert Symbol is an always-on display on top of the AI prototyping tool,
    displaying the alert level of a user's prompt. Every time the user runs
    their prompt, the Alert Symbol updates the alert level using the new prompt.
    Based on the computed alert level, there are three modes, each characterized
    by a progressively more attention-grabbing style. Thus, Farsight only
    disrupts AI prototypers' flow when their prompts require more caution.

  - >
    To categorize the potential harms that might arise from users' prompts,
    Farsight employs a novel technique that uses the <strong>similarity between
    the prompt and previously documented AI incident reports</strong> as a proxy
    for the prompt's alert level. First, we use LLM to extract high-dimensional
    latent representations (embeddings) of all AI incident reports indexed in
    the <a href='https://incidentdatabase.ai/' target='_blank'
    rel='noreferrer'>AI Incident Database</a>, which includes more than 3k
    community-curated news reports about AI failures and harms. Then, we extract
    the embedding of the user's prompt and compute pairwise cosine distances
    between the prompt embedding and AI incident report embeddings. We label
    each incident report as <code class="code">irrelevant</code>, <code
    class="code">remotely relevant</code>, and <code class="code">moderately
    relevant</code> based on two distance thresholds 0.69 and 0.75. We determine
    these two thresholds from an experiment with 1k random prompts.

  - >
    Finally, we show the numbers of AI incidents that are classified as <code
    class="code">remotely relevant</code> in an orange circle and <code
    class="code">moderately relevant</code> in a red circle as a proxy of the
    prompt's potential risk. In other words, we consider a prompt to have a
    higher risk <strong>if many AI incident reports are semantically and
    syntactically similar to it.</strong> See Figure 1 for a visualization of
    the embeddings of AI incident reports and prompts written by real users. An
    interactive version is available at <a
    href='https://poloclub.github.io/wizmap/?dataURL=https://huggingface.co/datasets/xiaohk/embeddings/resolve/main/ai-incident/data.ndjson&gridURL=https://huggingface.co/datasets/xiaohk/embeddings/resolve/main/ai-incident/grid.json'
    target='_blank' rel='noreferrer'>WizMap</a>.

usageIncident:
  - >
    After a user clicks the Alert Symbol, the Awareness Sidebar expands from the
    right of the AI prototyping tool, highlighting the potential consequences of
    AI applications or features that are based on the user's current prompt.

  - >
    To encourage users to consider potential risks associated with their
    prompts, the Incident Panel highlights news headlines of AI incidents that
    are relevant to the user's prompt. These incidents comprise the top 30
    incident reports that are classified as <code>moderately relevant</code> or
    <code>remotely relevant</code>, sorted in reverse order based on their
    embedding cosine distances to the embedding of the user's prompt. The
    thumbnails are color-coded based on the incident's relevancy level. Users
    can click the headline or the thumbnail to open the original incident report
    in a new tab. These real AI incidents can function as cautionary tales
    reminding users of potential AI harms.

usageUseCase:
  - >
    To help users imagine how their AI prototype may be used in AI applications
    or features, the Use Case Panel presents a diverse set of potential use
    cases that are generated by an LLM. Each use case is shown as a sentence
    describing how a particular group of end-users could use this AI application
    in a specific context. For example, for a writing tutor prompt, a potential
    use case can be “teachers use it to provide feedback on student writing.” We
    also use an LLM to generate a potential harm that could occur within that
    use case, shown below the use case sentence. For example, a harm for the
    teacher feedback use case can be “students may feel like they are not
    getting personalized feedback from their teachers.” We use few-shot learning
    to prompt the LLM to generate use cases and harms, whereas we generate use
    cases, stakeholders, and harms in Harm Envisioner.

  - >
    To help users assess and organize use cases and harms, we also leverage an
    LLM to categorize each use case as <span class="label
    label-intended">intended</span>, <span class="label
    label-high-stakes">high-stakes</span>, or <span class="label
    label-misuse">misuse</span>, although we acknowledge that these may vary by
    use cases, development and deployment contexts, as well as relevant policies
    or regulatory frameworks in various jurisdictions. These three categories
    are introduced by <a
    href='https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4ZzOI'
    target='_blank' rel='noreferrer'>responsible AI researchers</a> to help ML
    developers structure their harm envisioning process. The <span class="label
    label-intended">intended</span> use cases align with the
    development target use cases. The <span class="label
    label-high-stakes">high-stakes</span> use cases encompass those that may
    arise in high-stakes domains, such as medicine, finance, and the law. The
    <span class="label label-misuse">misuse</span> category includes scenarios
    where malicious actors exploit the AI application to cause harms. The Use
    Case Panel organizes use cases and harms into three tabs based on their
    categories. The first tab, "mix", provides an overview by showing one use
    case and its corresponding harm from each of the other tabs.

usageHarmEnvisioner:
  - >
    Both the Alert Symbol and the Awareness Sidebar provide easy-to-understand
    in-context reminders to help users reflect on potential harms associated
    with their prompts. However, instead of passively reading AI incident
    reports and LLM-generated content, some users desire to actively edit and
    add new <span class="colored use-case">use cases</span>, <span class="colored
    stakeholder">stakeholders</span>, and <span class="colored
    harm">harms</span>. Also, active participation—a key factor in
    learning—may help foster AI prototypers' ability to independently identify
    harms. Therefore, we design Harm Envisioner to support users in actively
    envisioning potential harms associated with their prompts.

  - >
    After clicking the "Envision Consequences & Harms" button in the Awareness
    Sidebar, Harm Envisioner appears as a pop-up window on top of the
    prompt-crafting tool. It begins with a text box filled with an LLM-generated
    <span class="colored summary">summary</span> of a user's prompt. The user is
    prompted to revise the <span class="colored summary">summary</span> to align with
    the target task in their prompt. Next, the window transitions into an
    interactive node-link tree visualization, where the user can pan and zoom to
    navigate the view. First, the window shows the user's <span class="colored
    summary">prompt summary</span> as the root of the tree which is visualized
    as a text box. The user can click the root node to ask AI to generate
    potential use cases of an AI application based on the user's prompt, and the
    use cases are visualized as the root's children nodes. Similarly, users can
    click a generated node to let AI generate its children nodes (stakeholders
    and then harms). There is a max of four layers, following an order of the
    user's <span class="colored summary">prompt summary</span> → <span
    class="colored use-case">use cases</span> → <span class="colored
    stakeholder">stakeholders</span> → <span class="colored harm">harms</span>.
    This layer order reflects the recommended harm envisioning workflow in
    responsible AI literature and helps users comprehend and organize diverse
    harms across different contexts.

figures:
  alert:
    url: https://github.com/xiaohk/xiaohk/assets/15007159/8f15498c-8e2a-44d8-b569-73df5b16dabe
    caption: >
      A visualization of the PaLM 2 embeddings of 3,474 <a
      href='https://incidentdatabase.ai/' target='_blank' rel='noreferrer'>AI
      incident reports</a> (blue dots and contour) and 153 <a
      href='https://github.com/f/awesome-chatgpt-prompts' target='_blank'
      rel='noreferrer'>Awesome ChatGPT Prompts</a> (red dots and contour). The
      embeddings' dimensions were reduced from 768 to 2 using <a
      href='https://umap-learn.readthedocs.io/en/latest/' target='_blank'
      rel='noreferrer'>UMAP</a>. The rectangles and labels show the summaries of
      AI incident reports in high-density embedding neighborhoods.  The overlap
      between the red and blue contours indicates that user prompts can be in
      close proximity to AI incident reports in the 2D embedding space. This
      observation inspires us to use high-dimensional embedding similarities to
      calculate the alert levels in Farsight. Note that in this example, the 153
      user prompts form a cluster due to the primary focus of AwesomeGPT prompts
      on conversational agents. The distribution of our 1,000 internal prompts
      (featuring classification, translation, code generation, etc.) is more
      spread out. For an interactive version of this visualization, visit <a
      href='https://poloclub.github.io/wizmap/?dataURL=https://huggingface.co/datasets/xiaohk/embeddings/resolve/main/ai-incident/data.ndjson&gridURL=https://huggingface.co/datasets/xiaohk/embeddings/resolve/main/ai-incident/grid.json'
      target='_blank' rel='noreferrer'>WizMap</a>.
